{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007164b6-b3ad-4565-b6f4-7205de14cd93",
   "metadata": {},
   "source": [
    "\n",
    "CSYE7105 Parallel Machine Learning & AI\n",
    "\n",
    "Intructor: Dr. Handan Liu\n",
    "\n",
    "Lecture: Data Parallelism and Model Parallelism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2de23d-554d-4474-97d1-e6ddb85bc13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b3deb043-4fc0-4c71-94f4-75bdbb1405c9",
   "metadata": {},
   "source": [
    "\n",
    "Basic Use Case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65796f8e-0d99-4dfc-bfce-b132c4015b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec70c37f-d64f-414b-ad0e-3b284181a400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05d09a-2eaa-4ea2-8b83-01a248ad40d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6147b29-803e-49a8-bff8-ea04bc299145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, size, run, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    run(rank, size)\n",
    "    \n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650dba0a-b549-48cb-9365-5ae85fd53787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22016889-c001-4b08-886c-5c2f5bfc9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Running basic DDP example on rank {rank}.\\n\")\n",
    "\n",
    "    # create model and move it to GPU with id rank\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b9342-1325-482f-b6bf-d997e64e91dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1c4943-c6b0-4445-87e1-59b06880d7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic DDP example on rank 0.\n",
      "\n",
      "Running basic DDP example on rank 1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    #mp.set_start_method(\"spawn\")\n",
    "    for rank in range(size):\n",
    "        p = mp.Process(target=setup, args=(rank, size, demo_basic))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db5476-3ea2-4fdb-b67a-3903ece94203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a242af-e7c3-4b78-ae7d-2c6dd80d8f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa802a9-170b-46b8-b04d-fae98624c817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1a236b30-80d2-4e0d-9595-f44b1f16c20b",
   "metadata": {},
   "source": [
    "\n",
    "Save and Load Checkpoints\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "397414d5-0425-46ae-b7fe-b9b9a17c2c86",
   "metadata": {},
   "source": [
    "Itâ€™s common to use torch.save and torch.load to checkpoint modules during training and recover from checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1156ef-4dfd-4ab4-bd47-9ff0f0f19e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_checkpoint(rank, world_size):\n",
    "    print(f\"Running DDP checkpoint example on rank {rank}.\\n\")\n",
    "\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "\n",
    "    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n",
    "    if rank == 0:\n",
    "        \"\"\"All processes should see same parameters as they all start from same\n",
    "           random parameters and gradients are synchronized in backward passes.\n",
    "           Therefore, saving it in one process is sufficient.\"\"\"\n",
    "        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n",
    "\n",
    "    # Use a barrier() to make sure that process 1 loads the model after process 0 saves it.\n",
    "    dist.barrier()\n",
    "    \n",
    "    # configure map_location properly\n",
    "    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n",
    "    ddp_model.load_state_dict(\n",
    "        torch.load(CHECKPOINT_PATH, map_location=map_location))\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \"\"\"Not necessary to use a dist.barrier() to guard the file deletion below\n",
    "    as the AllReduce ops in the backward pass of DDP already served as a synchronization.\"\"\"\n",
    "\n",
    "    if rank == 0:\n",
    "        os.remove(CHECKPOINT_PATH)\n",
    "\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a980f95e-b296-4ac3-84c3-81fa3dcf6b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDP checkpoint example on rank 0.\n",
      "\n",
      "Running DDP checkpoint example on rank 1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    #mp.set_start_method(\"spawn\")\n",
    "    for rank in range(size):\n",
    "        p = mp.Process(target=setup, args=(rank, size, demo_checkpoint))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52250559-21d9-455b-bb97-2c2a29f05890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd0ef8-e642-458f-897d-af7962d14a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca38f4-94de-4918-a718-6e92bce54105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2f60d21a-1383-46b8-8bcc-2f54cf002ccf",
   "metadata": {},
   "source": [
    "\n",
    "Combining DDP with Model Parallelism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796b2dd3-d50a-4831-af49-b70e6c78338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyMpModel(nn.Module):\n",
    "    def __init__(self, dev0, dev1):\n",
    "        super(ToyMpModel, self).__init__()\n",
    "        self.dev0 = dev0\n",
    "        self.dev1 = dev1\n",
    "        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.dev0)\n",
    "        x = self.relu(self.net1(x))\n",
    "        x = x.to(self.dev1)\n",
    "        return self.net2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ffa2e9-6c91-4cc8-9935-993c04d3eb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f795b77c-43c3-4b33-a76b-8065bdf7dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_model_parallel(rank, world_size):\n",
    "    print(f\"Running DDP with model parallel example on rank {rank}.\\n\")\n",
    "    \n",
    "    # setup mp_model and devices for this process\n",
    "    dev0 = rank * 2\n",
    "    dev1 = rank * 2 + 1\n",
    "    mp_model = ToyMpModel(dev0, dev1)\n",
    "    ddp_mp_model = DDP(mp_model)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # outputs will be on dev1\n",
    "    outputs = ddp_mp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(dev1)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cea75c7-5973-4737-98c7-75ee552a058a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gpus = torch.cuda.device_count()\n",
    "n_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4132aa68-14d4-49c3-9cf4-f1551665be89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDP with model parallel example on rank 3.\n",
      "Running DDP with model parallel example on rank 1.\n",
      "Running DDP with model parallel example on rank 2.\n",
      "Running DDP with model parallel example on rank 0.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-6:\n",
      "Process Process-8:\n",
      "Process Process-7:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_220311/3327019036.py\", line 6, in setup\n",
      "    run(rank, size)\n",
      "  File \"/tmp/ipykernel_220311/3327019036.py\", line 6, in setup\n",
      "    run(rank, size)\n",
      "  File \"/tmp/ipykernel_220311/3327019036.py\", line 6, in setup\n",
      "    run(rank, size)\n",
      "  File \"/tmp/ipykernel_220311/3327019036.py\", line 6, in setup\n",
      "    run(rank, size)\n",
      "  File \"/tmp/ipykernel_220311/242955160.py\", line 7, in demo_model_parallel\n",
      "    mp_model = ToyMpModel(dev0, dev1)\n",
      "  File \"/tmp/ipykernel_220311/242955160.py\", line 7, in demo_model_parallel\n",
      "    mp_model = ToyMpModel(dev0, dev1)\n",
      "  File \"/tmp/ipykernel_220311/242955160.py\", line 7, in demo_model_parallel\n",
      "    mp_model = ToyMpModel(dev0, dev1)\n",
      "  File \"/tmp/ipykernel_220311/242955160.py\", line 7, in demo_model_parallel\n",
      "    mp_model = ToyMpModel(dev0, dev1)\n",
      "  File \"/tmp/ipykernel_220311/3514889282.py\", line 6, in __init__\n",
      "    self.net1 = torch.nn.Linear(10, 10).to(dev0)\n",
      "  File \"/tmp/ipykernel_220311/3514889282.py\", line 6, in __init__\n",
      "    self.net1 = torch.nn.Linear(10, 10).to(dev0)\n",
      "  File \"/tmp/ipykernel_220311/3514889282.py\", line 6, in __init__\n",
      "    self.net1 = torch.nn.Linear(10, 10).to(dev0)\n",
      "  File \"/tmp/ipykernel_220311/3514889282.py\", line 6, in __init__\n",
      "    self.net1 = torch.nn.Linear(10, 10).to(dev0)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 927, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 927, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 927, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 927, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 602, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 925, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 925, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 925, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 925, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 207, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 207, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 207, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "  File \"/home/flyingsky2007/.conda/envs/py2022/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 207, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    size = 4\n",
    "    processes = []\n",
    "    #mp.set_start_method(\"spawn\")\n",
    "    for rank in range(size):\n",
    "        p = mp.Process(target=setup, args=(rank, size, demo_model_parallel))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "836412fb-38c1-40ac-aa4e-04a4ae219113",
   "metadata": {},
   "source": [
    "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca335d8c-dac6-4778-a93e-97dedaf828e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8957d408-14c8-49f7-ab57-062f520ef4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e228a-e7d7-4ba9-a1f5-6fb4bf4b6b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
