{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d326a8f-7b8d-4f03-8281-79405e23d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98f6bcb3-e760-4f1d-9ca3-d496e56e8015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup(rank, world_size, run, backend='nccl'):\n",
    "#     # Setup for DDP: OS environment variables and initialization\n",
    "#     os.environ['MASTER_ADDR'] = 'localhost'\n",
    "#     os.environ['MASTER_PORT'] = '12355'\n",
    "#     if not dist.is_initialized():\n",
    "#         dist.init_process_group(backend=backend, rank=rank, world_size=world_size)\n",
    "#     run(rank, size)\n",
    "#     torch.cuda.set_device(rank)\n",
    "    \n",
    "# def cleanup():\n",
    "#     dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf27690c-4e58-4c3c-b98e-7441d73dc6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(rank, world_size):\n",
    "    print(f\"Starting process on rank {rank}.\")\n",
    "\n",
    "    # Setup for Distributed Data Parallel (DDP)\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # Model, dataset, dataloader setup\n",
    "    model = models.resnet50(pretrained=True).cuda(rank)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 5)  # Example for 5 classes\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # Example dataset and DataLoader\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    dataset = datasets.FakeData(size=100, transform=transform)\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=10)\n",
    "\n",
    "    # Example training loop\n",
    "    criterion = nn.CrossEntropyLoss().cuda(rank)\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.01, momentum=0.9)\n",
    "    for epoch in range(2):  # example epoch count\n",
    "        ddp_model.train()\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.cuda(rank), target.cuda(rank)\n",
    "            optimizer.zero_grad()\n",
    "            output = ddp_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Cleanup\n",
    "    dist.destroy_process_group()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66f24823-530b-4f23-8217-f1e74c4f4a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World size: 2\n"
     ]
    }
   ],
   "source": [
    "world_size = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "print(f\"World size: {world_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8658efa-ee4b-46f4-9b02-f112c3b2af0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World size: 2\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 1 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorld size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworld_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Spawn processes using torch.multiprocessing.spawn\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m         \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the train function to be executed after setup\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m         \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m         \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:241\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    235\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m start_method\n\u001b[1;32m    239\u001b[0m     )\n\u001b[1;32m    240\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:148\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, name),\n\u001b[1;32m    142\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m             signal_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, exitcode),\n\u001b[1;32m    150\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    151\u001b[0m             error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    152\u001b[0m             exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    155\u001b[0m original_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_queues[error_index]\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    156\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 1 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    world_size = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "    print(f\"World size: {world_size}\")\n",
    "    \n",
    "    # Spawn processes using torch.multiprocessing.spawn\n",
    "    mp.spawn(setup,\n",
    "             args=(world_size, train),  # Pass the train function to be executed after setup\n",
    "             nprocs=world_size,\n",
    "             join=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826124f0-5360-48dd-89c0-64294f78eabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "372ea6aa-1faf-4987-ae62-e464b0094f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic DDP example on rank 0.\n",
      "Running basic DDP example on rank 1.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-17:\n",
      "Process Process-18:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-27-eba5693bc2f1>\", line 18, in setup\n",
      "    run(rank, size)\n",
      "  File \"<ipython-input-27-eba5693bc2f1>\", line 18, in setup\n",
      "    run(rank, size)\n",
      "  File \"<ipython-input-27-eba5693bc2f1>\", line 39, in demo_basic\n",
      "    model = ToyModel().to(rank)\n",
      "  File \"<ipython-input-27-eba5693bc2f1>\", line 39, in demo_basic\n",
      "    model = ToyModel().to(rank)\n",
      "  File \"/home/hindupur.v/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1152, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/hindupur.v/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 802, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/hindupur.v/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 825, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/hindupur.v/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1150, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "  File \"/home/hindupur.v/.local/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 288, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "  File \"/home/hindupur.v/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1152, in to\n",
      "    return self._apply(convert)\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "  File \"/home/hindupur.v/.local/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 288, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "  File \"/home/hindupur.v/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 802, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/hindupur.v/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 825, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/hindupur.v/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1150, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def setup(rank, size, run, backend='nccl'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend=backend, rank=rank, world_size=size)\n",
    "    run(rank, size)\n",
    "    \n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    \n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Running basic DDP example on rank {rank}.\\n\")\n",
    "\n",
    "    # create model and move it to GPU with id rank\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    #mp.set_start_method(\"spawn\")\n",
    "    for rank in range(size):\n",
    "        p = mp.Process(target=setup, args=(rank, size, demo_basic))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb62f33e-8f28-4048-95c4-fa70b705ce2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd6ddd0-d991-40d9-928e-7fa159482ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ee673-9557-458d-b5bb-4606c1d6411a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c70512fb-0b74-4415-924e-72d01e22b770",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m     mp\u001b[38;5;241m.\u001b[39mspawn(train, args\u001b[38;5;241m=\u001b[39m(world_size,), nprocs\u001b[38;5;241m=\u001b[39mworld_size, join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 63\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     62\u001b[0m     world_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:241\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    235\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m start_method\n\u001b[1;32m    239\u001b[0m     )\n\u001b[1;32m    240\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:148\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, name),\n\u001b[1;32m    142\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m             signal_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, exitcode),\n\u001b[1;32m    150\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    151\u001b[0m             error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    152\u001b[0m             exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    155\u001b[0m original_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_queues[error_index]\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    156\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    # Configure your dataset and DataLoader here\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    data_dir = '/home/hindupur.v/varsha_hpp/dataset/lung_colon_image_set'\n",
    "    dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, sampler=sampler)\n",
    "\n",
    "    # Model setup\n",
    "    model = resnet50(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(num_ftrs, 5) \n",
    "    model = model.cuda(rank)\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda(rank)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(10):  # Example: 10 epochs\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.cuda(rank)\n",
    "            labels = labels.cuda(rank)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d102a5e-f5f8-4857-85f0-cc2c36df703f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9557bc2-b735-4c5e-b9df-550aa012dfd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "060a4cf5-77cf-40cd-bba6-70d80ef31d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LungColonCancerDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # Load dataset\n",
    "        for label in os.listdir(data_dir):\n",
    "            class_dir = os.path.join(data_dir, label)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img in os.listdir(class_dir):\n",
    "                    self.samples.append((os.path.join(class_dir, img), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transformations\n",
    "transforms = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aeaa7ff-e486-4d51-85d5-434036eb57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from lung_colon_cancer_dataset import LungColonCancerDataset  # Hypothetical dataset class\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import os\n",
    "from torchvision import models\n",
    "\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        train_data: DataLoader,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        gpu_id: int,\n",
    "        save_every: int,\n",
    "    ) -> None:\n",
    "        self.gpu_id = gpu_id\n",
    "        self.model = model.to(gpu_id)\n",
    "        self.train_data = train_data\n",
    "        self.optimizer = optimizer\n",
    "        self.save_every = save_every\n",
    "        self.model = DDP(model, device_ids=[gpu_id])\n",
    "\n",
    "    # Other methods remain unchanged\n",
    "\n",
    "def load_train_objs():\n",
    "#   train_set = LungColonCancerDataset()  # Adjusted for your dataset\n",
    "    data_dir = '/home/hindupur.v/varsha_hpp/dataset/lung_colon_image_set'\n",
    "    train_set = LungColonCancerDataset(data_dir=data_dir, transform=transforms)\n",
    "    model = models.resnet50(pretrained=True)  # Using ResNet50 as an example\n",
    "    num_ftrs = model.fc.in_features\n",
    "    num_classes = 5  # Adjust the number of classes accordingly\n",
    "    model.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    return train_set, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5f60215-fccf-4383-a795-5469479edf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):\n",
    "    ddp_setup(rank, world_size)\n",
    "    dataset, model, optimizer = load_train_objs()\n",
    "    train_data = prepare_dataloader(dataset, batch_size)\n",
    "    trainer = Trainer(model, train_data, optimizer, rank, save_every)\n",
    "    trainer.train(total_epochs)\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1cde9f-183f-4595-a50a-313b61f11f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214b31b-d3b9-415c-a672-a6ba332e9f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22562d6-c7c3-43de-b86d-5b853b272124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647984a6-84e0-45f0-8e86-27bece9cc76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99d9f50-f89d-4b25-914a-e445d6190c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42465978-e185-4a5c-96ba-8904de0432d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "class LungColonCancerDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super(LungColonCancerDataset, self).__init__(root=root_dir, transform=transform)\n",
    "\n",
    "# Assuming your dataset is organized correctly and transforms are defined\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    data_dir = '/home/hindupur.v/varsha_hpp/dataset/lung_colon_image_set/'\n",
    "    train_dataset = LungColonCancerDataset(root_dir=os.path.join(data_dir, 'train'), transform=transform)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, sampler=train_sampler)\n",
    "\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 5)  # Assuming 5 classes\n",
    "    model = model.cuda(rank)\n",
    "    ddp_model = nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    for epoch in range(10):  # Adjust the number of epochs\n",
    "        ddp_model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.cuda(rank)\n",
    "            labels = labels.cuda(rank)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if rank == 0:  # Only print from the first process\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d15e8-4ceb-49de-b4c5-08ef2132d468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hindupur.v/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hindupur.v/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hindupur.v/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/hindupur.v/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3714846670627594\n",
      "Epoch 2, Loss: 0.007094573229551315\n",
      "Epoch 3, Loss: 0.0030669146217405796\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    world_size = torch.cuda.device_count()\n",
    "    processes = []\n",
    "    for rank in range(world_size):\n",
    "        p = mp.Process(target=train, args=(rank, world_size))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6a307-26ee-4dc6-9dce-f8566ad06e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
